package org.kson.parser

import org.kson.parser.TokenType.*
import org.kson.parser.messages.Message
import kotlin.test.Test
import kotlin.test.assertEquals
import kotlin.test.assertFalse

class LexerTest {
    /**
     * Assertion helper for testing [source] produces the sequence of [expectedTokenTypes].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenTypes is the list of expected types for the resulting tokens
     * @param message optionally pass a custom failure message for this assertion
     *
     * Returns the list of whole [Token]s for further validation
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenTypes: List<TokenType>,
        message: String? = null,
        testGapFreeLexing: Boolean = false
    ): List<Token> {
        val messageSink = MessageSink()
        val actualTokens = Lexer(source, messageSink, testGapFreeLexing).tokenize()
        val actualTokenTypes = actualTokens.map { it.tokenType }.toMutableList()

        // automatically clip off the always-trailing EOF so test-writers don't need to worry about it
        val eof = actualTokenTypes.removeLast()
        if (eof != EOF) {
            throw Exception("Tokenize should always produce a list of tokens ending in EOF... what's going on?")
        } else {
            val eofToken = actualTokens.last()

            // ensure EOF renders how we want when we render token lists to strings
            assertEquals("", eofToken.lexeme.text, "EOF Token's raw text should be empty (can't render an EOF)")
            assertEquals("", eofToken.value, "EOF Token's value should be empty (can't render an EOF)")
        }

        assertFalse(
            messageSink.hasErrors(),
            "Should not have lexing errors, got:\n\n" + LoggedMessage.print(messageSink.loggedMessages())
        )
        assertEquals(
            expectedTokenTypes,
            actualTokenTypes,
            message
        )

        return actualTokens
    }

    /**
     * Assertion helper to validate [Location]s generated by [Lexer].
     *
     * This test ensures that lexing [source] produces a sequence of tokens that matches the
     * given [expectedTokenLocationPairs].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenLocationPairs a list of [TokenType]/[Location] pairs that, by position, must match the
     *                                   [TokenType]/[Location] pairs produced by tokenizing [source]
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenLocationPairs: List<Pair<TokenType, Location>>,
        testGapFreeLexing: Boolean = false
    ) {
        val tokens = assertTokenizesTo(source, expectedTokenLocationPairs.map { it.first }, null, testGapFreeLexing)
        expectedTokenLocationPairs.forEachIndexed { index, tokenLocationPair ->
            val (tokenType, location) = tokenLocationPair
            assertEquals(
                location,
                tokens[index].lexeme.location,
                "Incorrect location for token of type $tokenType at index $index of the lexed tokens\n"
            )
        }
    }

    /**
     * Assertion helper to validate Values generated by [Lexer].
     *
     * This test ensures that lexing [source] produces a sequence of tokens that matches the
     * given [expectedTokenTypeValuePairs].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenTypeValuePairs a list of [TokenType]/[Any] pairs that, by position, must match the
     *                                   [TokenType]/value pairs produced by tokenizing [source]
     */
    private fun assertTokenizesWithValues(
        source: String,
        expectedTokenTypeValuePairs: List<Pair<TokenType, Any>>
    ) {
        val tokens = assertTokenizesTo(source, expectedTokenTypeValuePairs.map { it.first })
        expectedTokenTypeValuePairs.forEachIndexed { index, tokenTypeValuePair ->
            val (tokenType, value) = tokenTypeValuePair
            assertEquals(
                value,
                tokens[index].value,
                "Incorrect value for token of type $tokenType at index $index of the lexed tokens\n"
            )
        }
    }

    /**
     * Assertion helper for testing that tokenizing [source] generates [expectedMessages].
     */
    private fun assertTokenizesWithMessages(source: String, expectedMessages: List<Message>) {
        val messageSink = MessageSink()
        Lexer(source, messageSink).tokenize()
        assertEquals(expectedMessages, messageSink.loggedMessages().map { it.message })
    }

    @Test
    fun testEmptySource() {
        assertTokenizesTo(
            "",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            " ",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            "\t\n",
            emptyList<TokenType>()
        )
    }

    /**
     * Ensure we're well-behaved on source which has no leading or trailing whitespace
     */
    @Test
    fun testNoWhitespaceSource() {
        assertTokenizesTo(
            "1",
            listOf(NUMBER)
        )
    }

    @Test
    fun testStringLiteralSource() {
        assertTokenizesTo(
            """
                "This is a string"
            """,
            listOf(STRING)
        )
    }

    @Test
    fun testNumberLiteralSource() {
        assertTokenizesWithValues(
            """
                42
                42E0
                42e0
                4.2E1
                420E-1
                4200e-2
                0.42e2
                0.42e+2
                42E+0
                00042E0
                -42
                -42E0
                -42e0
                -4.2E1
                -420E-1
                -4200e-2
                -0.42e2
                -0.42e+2
                -42E+0
                -00042E0
            """,
            listOf(
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, 42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0),
                Pair(NUMBER, -42.0)
            )
        )

        // error case
        assertTokenizesWithMessages(
            """
                420E
            """,
            listOf(Message.DANGLING_EXP_INDICATOR)
        )

        // error case
        assertTokenizesWithMessages(
            """
                420E-
            """,
            listOf(Message.DANGLING_EXP_INDICATOR)
        )
    }

    @Test
    fun testDanglingMinusSign() {
        assertTokenizesWithMessages(
            """
                -
            """,
            listOf(Message.DANGLING_DASH)
        )
    }

    @Test
    fun testBooleanLiteralSource() {
        assertTokenizesTo(
            """
                true
            """,
            listOf(TRUE)
        )

        assertTokenizesTo(
            """
                false
            """,
            listOf(FALSE)
        )
    }

    @Test
    fun testNilLiteralSource() {
        assertTokenizesTo(
            """
                null
            """,
            listOf(NULL)
        )
    }

    @Test
    fun testEmptyListSource() {
        assertTokenizesTo(
            """
                []
            """,
            listOf(BRACKET_L, BRACKET_R)
        )
    }

    @Test
    fun testListSource() {
        assertTokenizesTo(
            """
                ["a string"]
            """,
            listOf(BRACKET_L, STRING, BRACKET_R)
        )

        assertTokenizesTo(
            """
                [42, 43, 44]
            """,
            listOf(BRACKET_L, NUMBER, COMMA, NUMBER, COMMA, NUMBER, BRACKET_R)
        )
    }

    @Test
    fun testEmptyObjectSource() {
        assertTokenizesTo(
            """
                {}
            """,
            listOf(BRACE_L, BRACE_R)
        )
    }

    @Test
    fun testObjectSource() {
        assertTokenizesTo(
            """
                {
                    key: val
                    "string key": 66
                    hello: "y'all"
                }
            """,
            listOf(BRACE_L, IDENTIFIER, COLON, IDENTIFIER, STRING, COLON, NUMBER, IDENTIFIER, COLON, STRING, BRACE_R)
        )

        assertTokenizesTo(
            """
                key: val
                "string key": 66
                hello: "y'all"
            """,
            listOf(IDENTIFIER, COLON, IDENTIFIER, STRING, COLON, NUMBER, IDENTIFIER, COLON, STRING)
        )
    }

    @Test
    fun testComments() {
        assertTokenizesTo(
            """
                # a comment!
                key: val
                        # wahoo!  Another comment!
                     # follow-up comment with wacky indent ðŸ•º
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                COMMENT,
                IDENTIFIER,
                COLON,
                IDENTIFIER,
                COMMENT,
                COMMENT,
                STRING,
                COLON,
                NUMBER,
                IDENTIFIER,
                COLON,
                STRING
            )
        )
    }

    @Test
    fun testEmbedBlockSource() {
        assertTokenizesTo(
            """
                %%
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertTokenizesTo(
            """
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBEDDED_BLOCK, EMBED_END)
        )
    }

    @Test
    fun testEmbedBlockIndentTrimming() {
        val oneLineEmbedTokens = assertTokenizesTo(
            """
                %%
                this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals("this is a raw embed\n", oneLineEmbedTokens[1].value)

        val mulitLineEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals(
            """
                this is a multi-line
                    raw embed
            who's indent will be determined by
                            the leftmost line
            
            """.trimIndent(),
            mulitLineEmbedTokens[2].value
        )

        val mulitLineIndentedEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line,
                which is the end delimiter in this case
              %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals(
            """      this is a multi-line
          raw embed
  who's indent will be determined by
                  the leftmost line,
  which is the end delimiter in this case
""",
            mulitLineIndentedEmbedTokens[2].value
        )
    }

    @Test
    fun testEmbedBlockTrialingWhitespace() {
        val trailingNewlineTokens = assertTokenizesTo(
            """
                %%
                this should have a newline at the end
                %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals("this should have a newline at the end\n", trailingNewlineTokens[1].value)

        val trailingSpacesTokens = assertTokenizesTo(
            """
                %%
                this lovely embed
                    should have four trailing 
                    spaces and a newline at the end    
                %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals(
            """
            this lovely embed
                should have four trailing 
                spaces and a newline at the end    

            """.trimIndent(),
            trailingSpacesTokens[1].value
        )

        val zeroTrailingWhitespaceTokens = assertTokenizesTo(
            """
                %%
                    this on the other hand,
                    should have spaces but no newline at the end    %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals(
            "this on the other hand,\nshould have spaces but no newline at the end    ",
            zeroTrailingWhitespaceTokens[1].value
        )
    }

    @Test
    fun testEmbedBlockTrailingWhitespace() {
        assertTokenizesTo(
            // note the extra whitespace after the opening ```
            """
                %%   
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END),
            "should allow trailing whitespace after the opening '```'"
        )

        assertTokenizesTo(
            // note the extra whitespace after the opening ```
            """   
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBEDDED_BLOCK, EMBED_END),
            "should allow trailing whitespace after the opening '```embedTag'"
        )
    }

    @Test
    fun testEmbedBlockDanglingHash() {
        assertTokenizesWithMessages(
            """
            test: %
            """,
            listOf(Message.EMBED_BLOCK_DANGLING_HASH)
        )
    }

    @Test
    fun testEmbedBlockBadStart() {
        assertTokenizesWithMessages(
            """
            %% this can't be here
            because content must start on first line after opening ticks
            %%
            """,
            listOf(Message.EMBED_BLOCK_BAD_START)
        )

        assertTokenizesWithMessages(
            """
            %%embedTag this can't be here
            because content must start on first line after opening ticks+embed tag
            %%
            """,
            listOf(Message.EMBED_BLOCK_BAD_START)
        )
    }

    @Test
    fun testUnclosedEmbedBlock() {
        assertTokenizesWithMessages(
            """
            %%
            This embed block lacks its closing ticks
            """,
            listOf(Message.EMBED_BLOCK_NO_CLOSE)
        )
    }

    @Test
    fun testUnterminatedString() {
        assertTokenizesWithMessages(
            """
            "this string has no end quote
            """,
            listOf(Message.STRING_NO_CLOSE)
        )
    }

    @Test
    fun testIdentifierLexemeContent() {
        val tokens = assertTokenizesTo(
            """   
                a_key: "a_value"
            """,
            listOf(IDENTIFIER, COLON, STRING)
        )

        assertEquals("a_key", tokens[0].value)
        assertEquals("a_value", tokens[2].value)
    }

    @Test
    fun testTokenLocations() {
        assertTokenizesTo(
            """
            |{
            |    key: val
            |    list: [true, false]
            |    embed: %%
            |      multiline tokens
            |      should have correct
            |      Locations too
            |      %%
            |}
            """.trimMargin(),
            listOf(
                Pair(BRACE_L, Location(0, 0, 0, 1, 0, 1)),
                Pair(IDENTIFIER, Location(1, 4, 1, 7, 6, 9)),
                Pair(COLON, Location(1, 7, 1, 8, 9, 10)),
                Pair(IDENTIFIER, Location(1, 9, 1, 12, 11, 14)),
                Pair(IDENTIFIER, Location(2, 4, 2, 8, 19, 23)),
                Pair(COLON, Location(2, 8, 2, 9, 23, 24)),
                Pair(BRACKET_L, Location(2, 10, 2, 11, 25, 26)),
                Pair(TRUE, Location(2, 11, 2, 15, 26, 30)),
                Pair(COMMA, Location(2, 15, 2, 16, 30, 31)),
                Pair(FALSE, Location(2, 17, 2, 22, 32, 37)),
                Pair(BRACKET_R, Location(2, 22, 2, 23, 37, 38)),
                Pair(IDENTIFIER, Location(3, 4, 3, 9, 43, 48)),
                Pair(COLON, Location(3, 9, 3, 10, 48, 49)),
                Pair(EMBED_START, Location(3, 11, 3, 13, 50, 52)),
                Pair(EMBEDDED_BLOCK, Location(4, 0, 7, 6, 53, 128)),
                Pair(EMBED_END, Location(7, 6, 7, 8, 128, 130)),
                Pair(BRACE_R, Location(8, 0, 8, 1, 131, 132))
            )
        )
    }

    @Test
    fun testStringEscapes() {
        val tokens = assertTokenizesTo(
            """   
                "string with \"embedded\" quotes"
            """,
            listOf(STRING)
        )

        assertEquals("string with \"embedded\" quotes", tokens[0].value)
    }

    @Test
    fun testEmbeddedBlockHashEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                %%
                these double %\% ticks are embedded but escaped%%
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals("these double %% ticks are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testEmbeddedBlockDollarEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                $$
                these double $\$ dollars are embedded but escaped$$
            """,
            listOf(EMBED_START, EMBEDDED_BLOCK, EMBED_END)
        )

        assertEquals("these double $$ dollars are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testGapFreeLexing() {
        assertTokenizesTo(
            """
                key: val
            """,
            listOf(WHITESPACE, IDENTIFIER, COLON, WHITESPACE, IDENTIFIER, WHITESPACE),
            "Should include WHITESPACE tokens when lexing gap-free",
            true
        )

        assertTokenizesTo(
            """
                |  quoted: "string"
                |
            """.trimMargin(),
            listOf(
                Pair(WHITESPACE, Location(0, 0, 0, 2, 0, 2)),
                Pair(IDENTIFIER, Location(0, 2, 0, 8, 2, 8)),
                Pair(COLON, Location(0, 8, 0, 9, 8, 9)),
                Pair(WHITESPACE, Location(0, 9, 0, 10, 9, 10)),
                Pair(DOUBLE_QUOTE, Location(0, 10, 0, 11, 10, 11)),
                Pair(STRING, Location(0, 11, 0, 17, 11, 17)),
                Pair(DOUBLE_QUOTE, Location(0, 17, 0, 18, 17, 18)),
                Pair(WHITESPACE, Location(0, 18, 1, 0, 18, 19))
            ),
            true
        )
    }
}