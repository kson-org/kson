package org.kson.parser

import org.kson.parser.TokenType.*
import kotlin.test.Test
import kotlin.test.assertEquals

class LexerTest {
    /**
     * Assertion helper for testing [source] produces the sequence of [expectedTokenTypes].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenTypes is the list of expected types for the resulting tokens
     * @param message optionally pass a custom failure message for this assertion
     *
     * Returns the list of whole [Token]s (with the [EOF] clipped off) for further validation
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenTypes: List<TokenType>,
        message: String? = null,
        testGapFreeLexing: Boolean = false
    ): List<Token> {
        val actualTokens = Lexer(source, testGapFreeLexing).tokenize()

        val eofStrippedActualTokens = verifyAndClipEof(actualTokens)

        val actualTokenTypes = eofStrippedActualTokens.map { it.tokenType }.toMutableList()

        assertEquals(
            expectedTokenTypes,
            actualTokenTypes,
            message
        )

        return eofStrippedActualTokens
    }

    /**
     * Assertion helper to validate [Location]s generated by [Lexer].
     *
     * This test ensures that lexing [source] produces a sequence of tokens that matches the
     * given [expectedTokenLocationPairs].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenLocationPairs a list of [TokenType]/[Location] pairs that, by position, must match the
     *                                   [TokenType]/[Location] pairs produced by tokenizing [source]
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenLocationPairs: List<Pair<TokenType, Location>>,
        testGapFreeLexing: Boolean = false
    ) {
        val tokens = assertTokenizesTo(source, expectedTokenLocationPairs.map { it.first }, null, testGapFreeLexing)
        expectedTokenLocationPairs.forEachIndexed { index, tokenLocationPair ->
            val (tokenType, location) = tokenLocationPair
            assertEquals(
                location,
                tokens[index].lexeme.location,
                "Incorrect location for token of type $tokenType at index $index of the lexed tokens\n"
            )
        }
    }

    /**
     * Runs some assertions validating the [EOF] in the given [Token] list, and returns the same [Token] list
     * with the [EOF] clipped off to streamline further assertions on the [Token]s
     */
    private fun verifyAndClipEof(
        tokens: List<Token>
    ): List<Token> {
        // automatically clip off the always-trailing EOF so test-writers don't need to worry about it
        val eofToken = tokens.last()
        if (eofToken.tokenType != EOF) {
            throw Exception("Tokenize should always produce a list of tokens ending in EOF... what's going on?")
        }

        // assert EOF renders how we want when we render token lists to strings
        assertEquals("", eofToken.lexeme.text, "EOF Token's raw text should be empty (can't render an EOF)")

        return tokens.subList(0, tokens.size - 1)
    }

    @Test
    fun testEmptySource() {
        assertTokenizesTo(
            "",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            " ",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            "\t\n",
            emptyList<TokenType>()
        )
    }

    /**
     * Ensure we're well-behaved on source which has no leading or trailing whitespace
     */
    @Test
    fun testNoWhitespaceSource() {
        assertTokenizesTo(
            "1",
            listOf(NUMBER)
        )
    }

    @Test
    fun testStringLiteralSource() {
        assertTokenizesTo(
            """
                "This is a string"
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )
    }

    @Test
    fun testNumberLiteralSource() {
        assertTokenizesTo(
            """
                42
                42E0
                42e0
                4.2E1
                420E-1
                4200e-2
                0.42e2
                0.42e+2
                42E+0
                00042E0
                -42
                -42E0
                -42e0
                -4.2E1
                -420E-1
                -4200e-2
                -0.42e2
                -0.42e+2
                -42E+0
                -00042E0
            """,
            listOf(
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER
            )
        )

        // test malformed numbers also lex correctly (it's the parser's responsibility to report that they are malformed)
        assertTokenizesTo(
            """
                420E
                420E-
                -nope
                -10bb6.8
                9geee8.0e-f2
                9geee8.0ef-2
            """,
            listOf(NUMBER, NUMBER, NUMBER, NUMBER, NUMBER, NUMBER)
        )
    }

    @Test
    fun testBooleanLiteralSource() {
        assertTokenizesTo(
            """
                true
            """,
            listOf(TRUE)
        )

        assertTokenizesTo(
            """
                false
            """,
            listOf(FALSE)
        )
    }

    @Test
    fun testNilLiteralSource() {
        assertTokenizesTo(
            """
                null
            """,
            listOf(NULL)
        )
    }

    @Test
    fun testEmptyListSource() {
        assertTokenizesTo(
            """
                []
            """,
            listOf(SQUARE_BRACKET_L, SQUARE_BRACKET_R)
        )
    }

    @Test
    fun testSquareBracketListSource() {
        assertTokenizesTo(
            """
                ["a string"]
            """,
            listOf(SQUARE_BRACKET_L, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE, SQUARE_BRACKET_R)
        )

        assertTokenizesTo(
            """
                [42, 43, 44]
            """,
            listOf(SQUARE_BRACKET_L, NUMBER, COMMA, NUMBER, COMMA, NUMBER, SQUARE_BRACKET_R)
        )
    }

    @Test
    fun testDashListSource() {
        // ensure we test the boundary when `-` is the first char in the source (not whitespace, for instance)
        assertTokenizesTo(
            "- null",
            listOf(LIST_DASH, NULL)
        )

        assertTokenizesTo(
            """
                - "a string"
            """,
            listOf(LIST_DASH, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )

        assertTokenizesTo(
            """
                - 42
                - 43
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )

        // odd but kind of needs to be legal to keep the dash list element semantics straightforward...
        // a dash-list element is simply: a dash followed by a value
        assertTokenizesTo(
            """
                - 42 - 43 - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )
    }

    @Test
    fun testMixedLists() {
        assertTokenizesTo(
            """
                - 42
                - [2, 4]
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, SQUARE_BRACKET_L, NUMBER, COMMA, NUMBER, SQUARE_BRACKET_R, LIST_DASH, NUMBER)
        )

        // this must lex in spite of the fact it will parse with errors on the illegal list nesting
        assertTokenizesTo(
            """
                - 42
                - 
                    - "nope, not a sublist because it's ambiguous"
                    - "indentation is not significant!"
                - 44
            """,
            listOf(
                LIST_DASH,
                NUMBER,
                LIST_DASH,
                LIST_DASH,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                LIST_DASH,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                LIST_DASH,
                NUMBER
            )
        )
    }

    @Test
    fun testEmptyObjectSource() {
        assertTokenizesTo(
            """
                {}
            """,
            listOf(CURLY_BRACE_L, CURLY_BRACE_R)
        )
    }

    @Test
    fun testObjectSource() {
        assertTokenizesTo(
            """
                {
                    key: val
                    "string key": 66
                    hello: "y'all"
                }
            """,
            listOf(
                CURLY_BRACE_L,
                UNQUOTED_STRING,
                COLON,
                UNQUOTED_STRING,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                COLON,
                NUMBER,
                UNQUOTED_STRING,
                COLON,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                CURLY_BRACE_R
            )
        )

        assertTokenizesTo(
            """
                key: val
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                UNQUOTED_STRING,
                COLON,
                UNQUOTED_STRING,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                COLON,
                NUMBER,
                UNQUOTED_STRING,
                COLON,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE
            )
        )
    }

    @Test
    fun testComments() {
        assertTokenizesTo(
            """
                # a comment!
                key: val
                        # wahoo!  Another comment!
                     # follow-up comment with wacky indent ðŸ•º
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                UNQUOTED_STRING,
                COLON,
                UNQUOTED_STRING,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE,
                COLON,
                NUMBER,
                UNQUOTED_STRING,
                COLON,
                STRING_OPEN_QUOTE,
                STRING_CONTENT,
                STRING_CLOSE_QUOTE
            )
        )
    }

    @Test
    fun testEmbedBlockSource() {
        assertTokenizesTo(
            """
                %
                    this is a raw embed
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertTokenizesTo(
            """
                %sql
                    select * from something
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertTokenizesTo(
            """
                %:empty embed tag
                    select * from something
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_TAG_STOP, EMBED_METADATA, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertTokenizesTo(
            """
                %sql: metaTag
                    select * from something
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_TAG_STOP, EMBED_METADATA, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )
        assertTokenizesTo(
            """
                %sql: metaTag
                    select * from something
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_TAG_STOP, EMBED_METADATA, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )
    }

    @Test
    fun testEmbedBlockTrialingWhitespace() {
        val trailingNewlineTokens = assertTokenizesTo(
            """
                %
                this should have a newline at the end
                %%
            """.trimIndent(),
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertEquals("this should have a newline at the end\n", trailingNewlineTokens[2].lexeme.text)

        val trailingSpacesTokens = assertTokenizesTo(
            """
                %
                this lovely embed
                    should have four trailing 
                    spaces and a newline at the end    
                %%
            """.trimIndent(),
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertEquals(
            """
            this lovely embed
                should have four trailing 
                spaces and a newline at the end    

            """.trimIndent(),
            trailingSpacesTokens[2].lexeme.text
        )

        val zeroTrailingWhitespaceTokens = assertTokenizesTo(
            """
                %
                    this on the other hand,
                    should have spaces but no newline at the end    %%
            """.trimIndent(),
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertEquals(
            "    this on the other hand,\n    should have spaces but no newline at the end    ",
            zeroTrailingWhitespaceTokens[2].lexeme.text
        )
    }

    @Test
    fun testEmbedBlockTrailingWhitespace() {
        assertTokenizesTo(
            // note the extra whitespace after the opening `%`
            """
                %   
                    this is a raw embed
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM),
            "should allow trailing whitespace after the opening '%'"
        )

        assertTokenizesTo(
            // note the extra whitespace after the opening `%`
            """   
                %sql
                    select * from something
                %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM),
            "should allow trailing whitespace after the opening '%embedTag'"
        )
    }

    @Test
    fun testEmbedBlockDanglingDelim() {
        assertTokenizesTo(
            """
            test: %
            """,
            listOf(UNQUOTED_STRING, COLON, EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT)
        )
    }

    @Test
    fun testComplexEmbedTagWithWhitespace() {
        assertTokenizesTo(
            """
            %   this tag has spaces and funky characters ~!@#$%^&*()_+
            some sweet content
            %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertTokenizesTo(
            """
            %   this tag has spaces and funky characters ~!@#$%^&*()_+: followed by a meta tag
            some sweet content
            %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_TAG_STOP, EMBED_METADATA, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertTokenizesTo(
            """
            % tag: meta with escaped and unescaped: colon \:
            some sweet content
            %%
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_TAG, EMBED_TAG_STOP, EMBED_METADATA, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )
    }

    @Test
    fun testUnclosedEmbedBlock() {
        assertTokenizesTo(
            """
            %
            This embed block lacks its closing delimiter
            """,
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT)
        )
    }

    @Test
    fun testUnterminatedString() {
        assertTokenizesTo(
            """
            "this string has no end quote
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT),
            "should simply tokenize unterminated strings.  Errors are handled in parsing."
        )
    }

    @Test
    fun testUnterminatedAltString() {
        assertTokenizesTo(
            """
            'this string has no end quote
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT),
            "should simply tokenize unterminated strings.  Errors are handled in parsing."
        )
    }

    @Test
    fun testUnquotedStringLexemeContent() {
        val tokens = assertTokenizesTo(
            """   
                a_key: "a_value"
            """,
            listOf(UNQUOTED_STRING, COLON, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )

        assertEquals("a_key", tokens[0].lexeme.text)
        assertEquals("\"", tokens[2].lexeme.text)
        assertEquals("a_value", tokens[3].lexeme.text)
    }

    @Test
    fun testTokenLocations() {
        assertTokenizesTo(
            """
            |{
            |    key: val
            |    list: [true, false]
            |    embed: %
            |      multiline tokens
            |      should have correct
            |      Locations too
            |      %%
            |}
            """.trimMargin(),
            listOf(
                Pair(CURLY_BRACE_L, Location.create(0, 0, 0, 1, 0, 1)),
                Pair(UNQUOTED_STRING, Location.create(1, 4, 1, 7, 6, 9)),
                Pair(COLON, Location.create(1, 7, 1, 8, 9, 10)),
                Pair(UNQUOTED_STRING, Location.create(1, 9, 1, 12, 11, 14)),
                Pair(UNQUOTED_STRING, Location.create(2, 4, 2, 8, 19, 23)),
                Pair(COLON, Location.create(2, 8, 2, 9, 23, 24)),
                Pair(SQUARE_BRACKET_L, Location.create(2, 10, 2, 11, 25, 26)),
                Pair(TRUE, Location.create(2, 11, 2, 15, 26, 30)),
                Pair(COMMA, Location.create(2, 15, 2, 16, 30, 31)),
                Pair(FALSE, Location.create(2, 17, 2, 22, 32, 37)),
                Pair(SQUARE_BRACKET_R, Location.create(2, 22, 2, 23, 37, 38)),
                Pair(UNQUOTED_STRING, Location.create(3, 4, 3, 9, 43, 48)),
                Pair(COLON, Location.create(3, 9, 3, 10, 48, 49)),
                Pair(EMBED_OPEN_DELIM, Location.create(3, 11, 3, 12, 50, 51)),
                Pair(EMBED_PREAMBLE_NEWLINE, Location.create(3, 12, 4, 0, 51, 52)),
                Pair(EMBED_CONTENT, Location.create(4, 0, 7, 6, 52, 127)),
                Pair(EMBED_CLOSE_DELIM, Location.create(7, 6, 7, 8, 127, 129)),
                Pair(CURLY_BRACE_R, Location.create(8, 0, 8, 1, 130, 131))
            )
        )
    }

    @Test
    fun testStringEscapes() {
        val tokens = assertTokenizesTo(
            """   
                "string with 'unescaped' and \"embedded\" quotes"
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )

        // sanity check the tokens are lexing to what we expect
        assertEquals("string with 'unescaped' and ", tokens[1].lexeme.text)
        assertEquals("\\\"", tokens[2].lexeme.text)
        assertEquals("embedded", tokens[3].lexeme.text)
    }

    @Test
    fun testAltStringEscapes() {
        val tokens = assertTokenizesTo(
            """
                'string with "unescaped" and \'embedded\' quotes'
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )

        // sanity check the tokens are lexing to what we expect
        assertEquals("string with \"unescaped\" and ", tokens[1].lexeme.text)
        assertEquals("\\'", tokens[2].lexeme.text)
        assertEquals("embedded", tokens[3].lexeme.text)
    }

    @Test
    fun testStringWhitespaceAfterEscape() {
        assertTokenizesTo(
            """'string with \' whitespace after an escape'""",
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_CLOSE_QUOTE),
            testGapFreeLexing = true
        )

        assertTokenizesTo(
            """
                'string with \' whitespace after an escape'
            """,
            listOf(WHITESPACE, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_CLOSE_QUOTE, WHITESPACE),
            testGapFreeLexing = true
        )

        assertTokenizesTo(
            """
                'string with all whitespace after escape: \'     '
            """,
            listOf(WHITESPACE, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ESCAPE, STRING_CONTENT, STRING_CLOSE_QUOTE, WHITESPACE),
            testGapFreeLexing = true
        )
    }

    @Test
    fun testStringWhitespaceAfterIllegalControlCharacter() {
        assertTokenizesTo(
            """
                'string with   whitespace after an illegal escape char'
            """,
            listOf(WHITESPACE, STRING_OPEN_QUOTE, STRING_CONTENT, STRING_ILLEGAL_CONTROL_CHARACTER, STRING_CONTENT, STRING_CLOSE_QUOTE, WHITESPACE),
            testGapFreeLexing = true
        )
    }

    @Test
    fun testEmbeddedBlockDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                %
                these double %\% percents are embedded but escaped%%
            """.trimIndent(),
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertEquals("these double %\\% percents are embedded but escaped", singleEscapeTokens[2].lexeme.text)
    }

    @Test
    fun testEmbeddedBlockAltDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                $
                these double $\$ dollars are embedded but escaped$$
            """.trimIndent(),
            listOf(EMBED_OPEN_DELIM, EMBED_PREAMBLE_NEWLINE, EMBED_CONTENT, EMBED_CLOSE_DELIM)
        )

        assertEquals("these double $\\$ dollars are embedded but escaped", singleEscapeTokens[2].lexeme.text)
    }

    @Test
    fun testGapFreeLexing() {
        assertTokenizesTo(
            """
                # comment!
                key: val
            """,
            listOf(WHITESPACE, COMMENT, WHITESPACE, UNQUOTED_STRING, COLON, WHITESPACE, UNQUOTED_STRING, WHITESPACE),
            "Should include WHITESPACE tokens when lexing gap-free",
            true
        )

        assertTokenizesTo(
            """
                |  quoted: "string"
                |
            """.trimMargin(),
            listOf(
                Pair(WHITESPACE, Location.create(0, 0, 0, 2, 0, 2)),
                Pair(UNQUOTED_STRING, Location.create(0, 2, 0, 8, 2, 8)),
                Pair(COLON, Location.create(0, 8, 0, 9, 8, 9)),
                Pair(WHITESPACE, Location.create(0, 9, 0, 10, 9, 10)),
                Pair(STRING_OPEN_QUOTE, Location.create(0, 10, 0, 11, 10, 11)),
                Pair(STRING_CONTENT, Location.create(0, 11, 0, 17, 11, 17)),
                Pair(STRING_CLOSE_QUOTE, Location.create(0, 17, 0, 18, 17, 18)),
                Pair(WHITESPACE, Location.create(0, 18, 1, 0, 18, 19))
            ),
            true
        )
    }

    @Test
    fun testCommentPreservation() {
        val tokenList = assertTokenizesTo(
            """
               # a comment
               # another comment
               - 1
               
               # yet another comment
               - 2
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )

        val firstListDashToken = tokenList[0]
        assertEquals(
            2,
            firstListDashToken.comments.size,
            "should have both the comments on this list entry saved with this token"
        )
        assertEquals("# a comment", firstListDashToken.comments[0])
        assertEquals("# another comment", firstListDashToken.comments[1])

        val secondListDashToken = tokenList[2]
        assertEquals(1, secondListDashToken.comments.size)
        assertEquals("# yet another comment", secondListDashToken.comments[0])
    }

    @Test
    fun testTrailingCommentPreservationOnConstants() {
        val tokenList = assertTokenizesTo(
            """
                "stuff" # comment about stuff
            """,
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE)
        )

        val endQuoteToken = tokenList[2]
        assertEquals("# comment about stuff", endQuoteToken.comments[0])
    }

    @Test
    fun testTrailingCommentOnLists() {
        val tokenList = assertTokenizesTo(
            """
                [1, # trailing list comma
                2] # trailing list brace
            """,
            listOf(SQUARE_BRACKET_L, NUMBER, COMMA, NUMBER, SQUARE_BRACKET_R)
        )

        val commaToken = tokenList[2]
        assertEquals("# trailing list comma", commaToken.comments[0])

        val rightBracketToken = tokenList[4]
        assertEquals("# trailing list brace", rightBracketToken.comments[0])
    }

    @Test
    fun testHashInString() {
        assertTokenizesTo(
            "'# not a comment' # yes a coment",
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE, WHITESPACE, COMMENT),
            testGapFreeLexing = true
        )

        assertTokenizesTo(
            "'also # not a comment'# yes a comment",
            listOf(STRING_OPEN_QUOTE, STRING_CONTENT, STRING_CLOSE_QUOTE, COMMENT),
            testGapFreeLexing = true
        )
    }

    @Test
    fun testHashInEmbedTag() {
        assertTokenizesTo(
            "%%# should not be a comment",
            listOf(EMBED_OPEN_DELIM, EMBED_TAG),
            testGapFreeLexing = true
        )

        assertTokenizesTo(
            "%%also # should not be a comment",
            listOf(EMBED_OPEN_DELIM, EMBED_TAG),
            testGapFreeLexing = true
        )
    }

    @Test
    fun testlocationContainsPosition_targetStartsAtContainerStart() {
        val container = Location.create(
            firstLine = 2, firstColumn = 5,
            lastLine = 10, lastColumn = 15,
            startOffset = 20, endOffset = 100
        )
        val target = Coordinates(
            line = 2, column = 5,
        )

        assertEquals(
            true,
            Location.containsCoordinates(container, target),
            "Target starting exactly at container start should return true"
        )
    }

    @Test
    fun testLocationContainsLocation_targetStartsAtContainerEnd() {
        val container = Location.create(
            firstLine = 2, firstColumn = 5,
            lastLine = 10, lastColumn = 15,
            startOffset = 20, endOffset = 100
        )
        val target = Coordinates(
            line = 10, column = 15,
        )

        assertEquals(
            true,
            Location.containsCoordinates(container, target),
            "Target starting exactly at container end should return true"
        )
    }

    @Test
    fun testLocationContainsLocation_targetStartsBeforeContainerLine() {
        val container = Location.create(
            firstLine = 5, firstColumn = 0,
            lastLine = 10, lastColumn = 20,
            startOffset = 50, endOffset = 100
        )
        val target = Coordinates(3,10)

        assertEquals(
            false,
            Location.containsCoordinates(container, target),
            "Target starting before container line should return false"
        )
    }

    @Test
    fun testLocationContainsLocation_targetStartsAfterContainerLine() {
        val container = Location.create(
            firstLine = 5, firstColumn = 0,
            lastLine = 10, lastColumn = 20,
            startOffset = 50, endOffset = 100
        )
        val target = Coordinates(15,5)

        assertEquals(
            false,
            Location.containsCoordinates(container, target),
            "Target starting after container line should return false"
        )
    }

    @Test
    fun testLocationContainsLocation_sameLineTargetBeforeColumn() {
        val container = Location.create(
            firstLine = 5, firstColumn = 10,
            lastLine = 5, lastColumn = 20,
            startOffset = 50, endOffset = 60
        )
        val target = Coordinates(5,5)

        assertEquals(
            false,
            Location.containsCoordinates(container, target),
            "Target on same start line but before start column should return false"
        )
    }

    @Test
    fun testLocationContainsLocation_sameLineTargetAfterColumn() {
        val container = Location.create(
            firstLine = 5, firstColumn = 10,
            lastLine = 5, lastColumn = 20,
            startOffset = 50, endOffset = 60
        )
        val target = Coordinates(5,25)

        assertEquals(
            false,
            Location.containsCoordinates(container, target),
            "Target on same end line but after end column should return false"
        )
    }

    @Test
    fun testLocationContainsLocation_targetOnSameLineWithinBounds() {
        val container = Location.create(
            firstLine = 5, firstColumn = 10,
            lastLine = 5, lastColumn = 20,
            startOffset = 50, endOffset = 60
        )
        val target = Coordinates(5, 15)

        assertEquals(
            true,
            Location.containsCoordinates(container, target),
            "Target on same line and within column bounds should return true"
        )
    }

    @Test
    fun testLocationContainsLocation_multiLineContainer() {
        val container = Location.create(
            firstLine = 2, firstColumn = 5,
            lastLine = 8, lastColumn = 10,
            startOffset = 20, endOffset = 80
        )

        // Target on middle line (any column should be valid)
        val targetMiddle = Coordinates(5, 0)

        assertEquals(
            true,
            Location.containsCoordinates(container, targetMiddle),
            "Target on middle line should return true regardless of column"
        )
    }

    @Test
    fun testLocationContainsLocation_edgeCaseFirstLineBoundary() {
        val container = Location.create(
            firstLine = 5, firstColumn = 10,
            lastLine = 10, lastColumn = 5,
            startOffset = 50, endOffset = 100
        )

        // Target on first line, before column
        val targetBefore = Coordinates(5,9)

        assertEquals(
            false,
            Location.containsCoordinates(container, targetBefore),
            "Target on first line but before start column should return false"
        )

        // Target on first line, at column
        val targetAt = Coordinates(5,10)

        assertEquals(
            true,
            Location.containsCoordinates(container, targetAt),
            "Target on first line at start column should return true"
        )

        // Target on first line, after column
        val targetAfter = Coordinates(5,11)

        assertEquals(
            true,
            Location.containsCoordinates(container, targetAfter),
            "Target on first line after start column should return true"
        )
    }

    @Test
    fun testLocationContainsLocation_edgeCaseLastLineBoundary() {
        val container = Location.create(
            firstLine = 5, firstColumn = 10,
            lastLine = 10, lastColumn = 15,
            startOffset = 50, endOffset = 100
        )

        // Target on last line, before end column
        val targetBefore = Coordinates(10,10)

        assertEquals(
            true,
            Location.containsCoordinates(container, targetBefore),
            "Target on last line before end column should return true"
        )

        // Target on last line, at end column
        val targetAt = Coordinates(10,15)

        assertEquals(
            true,
            Location.containsCoordinates(container, targetAt),
            "Target on last line at end column should return true"
        )

        // Target on last line, after end column
        val targetAfter = Coordinates(10,16)

        assertEquals(
            false,
            Location.containsCoordinates(container, targetAfter),
            "Target on last line after end column should return false"
        )
    }
}
