package org.kson.parser

import org.kson.parser.TokenType.*
import org.kson.parser.messages.MessageType
import org.kson.parser.messages.MessageType.*
import kotlin.test.Test
import kotlin.test.assertEquals
import kotlin.test.assertFalse

class LexerTest {
    /**
     * Assertion helper for testing [source] produces the sequence of [expectedTokenTypes].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenTypes is the list of expected types for the resulting tokens
     * @param message optionally pass a custom failure message for this assertion
     *
     * Returns the list of whole [Token]s (with the [EOF] clipped off) for further validation
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenTypes: List<TokenType>,
        message: String? = null,
        testGapFreeLexing: Boolean = false
    ): List<Token> {
        val messageSink = MessageSink()
        val actualTokens = Lexer(source, messageSink, testGapFreeLexing).tokenize()

        val eofStrippedActualTokens = verifyAndClipEof(actualTokens)

        val actualTokenTypes = eofStrippedActualTokens.map { it.tokenType }.toMutableList()

        assertFalse(
            messageSink.hasErrors(),
            "Should not have lexing errors, got:\n\n" + LoggedMessage.print(messageSink.loggedMessages())
        )
        assertEquals(
            expectedTokenTypes,
            actualTokenTypes,
            message
        )

        return eofStrippedActualTokens
    }

    /**
     * Assertion helper to validate [Location]s generated by [Lexer].
     *
     * This test ensures that lexing [source] produces a sequence of tokens that matches the
     * given [expectedTokenLocationPairs].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenLocationPairs a list of [TokenType]/[Location] pairs that, by position, must match the
     *                                   [TokenType]/[Location] pairs produced by tokenizing [source]
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenLocationPairs: List<Pair<TokenType, Location>>,
        testGapFreeLexing: Boolean = false
    ) {
        val tokens = assertTokenizesTo(source, expectedTokenLocationPairs.map { it.first }, null, testGapFreeLexing)
        expectedTokenLocationPairs.forEachIndexed { index, tokenLocationPair ->
            val (tokenType, location) = tokenLocationPair
            assertEquals(
                location,
                tokens[index].lexeme.location,
                "Incorrect location for token of type $tokenType at index $index of the lexed tokens\n"
            )
        }
    }

    /**
     * Assertion helper for testing that tokenizing [source] generates [expectedMessageTypes].
     *
     * Returns the generated tokens for further validation
     */
    private fun assertTokenizesWithMessages(source: String, expectedMessageTypes: List<MessageType>): List<Token> {
        val messageSink = MessageSink()
        val tokens = Lexer(source, messageSink).tokenize()

        val eofStrippedTokens = verifyAndClipEof(tokens)

        assertEquals(expectedMessageTypes, messageSink.loggedMessages().map { it.message.type })
        return eofStrippedTokens
    }

    /**
     * Runs some assertions validating the [EOF] in the given [Token] list, and returns the same [Token] list
     * with the [EOF] clipped off to streamline further assertions on the [Token]s
     */
    private fun verifyAndClipEof(
        tokens: List<Token>
    ): List<Token> {
        // automatically clip off the always-trailing EOF so test-writers don't need to worry about it
        val eofToken = tokens.last()
        if (eofToken.tokenType != EOF) {
            throw Exception("Tokenize should always produce a list of tokens ending in EOF... what's going on?")
        }

        // assert EOF renders how we want when we render token lists to strings
        assertEquals("", eofToken.lexeme.text, "EOF Token's raw text should be empty (can't render an EOF)")
        assertEquals("", eofToken.value, "EOF Token's value should be empty (can't render an EOF)")

        return tokens.subList(0, tokens.size - 1)
    }

    @Test
    fun testEmptySource() {
        assertTokenizesTo(
            "",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            " ",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            "\t\n",
            emptyList<TokenType>()
        )
    }

    /**
     * Ensure we're well-behaved on source which has no leading or trailing whitespace
     */
    @Test
    fun testNoWhitespaceSource() {
        assertTokenizesTo(
            "1",
            listOf(NUMBER)
        )
    }

    @Test
    fun testStringLiteralSource() {
        assertTokenizesTo(
            """
                "This is a string"
            """,
            listOf(STRING_QUOTE, STRING, STRING_QUOTE)
        )
    }

    @Test
    fun testNumberLiteralSource() {
        assertTokenizesTo(
            """
                42
                42E0
                42e0
                4.2E1
                420E-1
                4200e-2
                0.42e2
                0.42e+2
                42E+0
                00042E0
                -42
                -42E0
                -42e0
                -4.2E1
                -420E-1
                -4200e-2
                -0.42e2
                -0.42e+2
                -42E+0
                -00042E0
            """,
            listOf(
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER
            )
        )

        // test malformed numbers also lex correctly (it's the parser's responsibility to report that they are malformed)
        assertTokenizesTo(
            """
                420E
                420E-
                -nope
                -10bb6.8
                9geee8.0e-f2
                9geee8.0ef-2
            """,
            listOf(NUMBER, NUMBER, NUMBER, NUMBER, NUMBER, NUMBER)
        )
    }

    @Test
    fun testBooleanLiteralSource() {
        assertTokenizesTo(
            """
                true
            """,
            listOf(TRUE)
        )

        assertTokenizesTo(
            """
                false
            """,
            listOf(FALSE)
        )
    }

    @Test
    fun testNilLiteralSource() {
        assertTokenizesTo(
            """
                null
            """,
            listOf(NULL)
        )
    }

    @Test
    fun testEmptyListSource() {
        assertTokenizesTo(
            """
                []
            """,
            listOf(BRACKET_L, BRACKET_R)
        )
    }

    @Test
    fun testBracketListSource() {
        assertTokenizesTo(
            """
                ["a string"]
            """,
            listOf(BRACKET_L, STRING_QUOTE, STRING, STRING_QUOTE, BRACKET_R)
        )

        assertTokenizesTo(
            """
                [42, 43, 44]
            """,
            listOf(BRACKET_L, NUMBER, COMMA, NUMBER, COMMA, NUMBER, BRACKET_R)
        )
    }

    @Test
    fun testDashListSource() {
        // ensure we test the boundary when `-` is the first char in the source (not whitespace, for instance)
        assertTokenizesTo(
            "- null",
            listOf(LIST_DASH, NULL)
        )

        assertTokenizesTo(
            """
                - "a string"
            """,
            listOf(LIST_DASH, STRING_QUOTE, STRING, STRING_QUOTE)
        )

        assertTokenizesTo(
            """
                - 42
                - 43
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )

        // odd but kind of needs to be legal to keep the dash list element semantics straightforward...
        // a dash-list element is simply: a dash followed by a value
        assertTokenizesTo(
            """
                - 42 - 43 - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )
    }

    @Test
    fun testMixedLists() {
        assertTokenizesTo(
            """
                - 42
                - [2, 4]
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, BRACKET_L, NUMBER, COMMA, NUMBER, BRACKET_R, LIST_DASH, NUMBER)
        )

        // this must lex in spite of the fact it will parse with errors on the illegal list nesting
        assertTokenizesTo(
            """
                - 42
                - 
                    - "nope, not a sublist because it's ambiguous"
                    - "indentation is not significant!"
                - 44
            """,
            listOf(
                LIST_DASH,
                NUMBER,
                LIST_DASH,
                LIST_DASH,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                LIST_DASH,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                LIST_DASH,
                NUMBER
            )
        )
    }

    @Test
    fun testEmptyObjectSource() {
        assertTokenizesTo(
            """
                {}
            """,
            listOf(BRACE_L, BRACE_R)
        )
    }

    @Test
    fun testObjectSource() {
        assertTokenizesTo(
            """
                {
                    key: val
                    "string key": 66
                    hello: "y'all"
                }
            """,
            listOf(
                BRACE_L,
                IDENTIFIER,
                COLON,
                IDENTIFIER,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                COLON,
                NUMBER,
                IDENTIFIER,
                COLON,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                BRACE_R
            )
        )

        assertTokenizesTo(
            """
                key: val
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                IDENTIFIER,
                COLON,
                IDENTIFIER,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                COLON,
                NUMBER,
                IDENTIFIER,
                COLON,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE
            )
        )
    }

    @Test
    fun testComments() {
        assertTokenizesTo(
            """
                # a comment!
                key: val
                        # wahoo!  Another comment!
                     # follow-up comment with wacky indent ðŸ•º
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                IDENTIFIER,
                COLON,
                IDENTIFIER,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE,
                COLON,
                NUMBER,
                IDENTIFIER,
                COLON,
                STRING_QUOTE,
                STRING,
                STRING_QUOTE
            )
        )
    }

    @Test
    fun testEmbedBlockSource() {
        assertTokenizesTo(
            """
                %%
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertTokenizesTo(
            """
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )
    }

    @Test
    fun testEmbedBlockIndentTrimming() {
        val oneLineEmbedTokens = assertTokenizesTo(
            """
                %%
                this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("this is a raw embed\n", oneLineEmbedTokens[1].value)

        val mulitLineEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """
                this is a multi-line
                    raw embed
            who's indent will be determined by
                            the leftmost line
            
            """.trimIndent(),
            mulitLineEmbedTokens[2].value
        )

        val mulitLineIndentedEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line,
                which is the end delimiter in this case
              %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """      this is a multi-line
          raw embed
  who's indent will be determined by
                  the leftmost line,
  which is the end delimiter in this case
""",
            mulitLineIndentedEmbedTokens[2].value
        )
    }

    @Test
    fun testEmbedBlockTrialingWhitespace() {
        val trailingNewlineTokens = assertTokenizesTo(
            """
                %%
                this should have a newline at the end
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("this should have a newline at the end\n", trailingNewlineTokens[1].value)

        val trailingSpacesTokens = assertTokenizesTo(
            """
                %%
                this lovely embed
                    should have four trailing 
                    spaces and a newline at the end    
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """
            this lovely embed
                should have four trailing 
                spaces and a newline at the end    

            """.trimIndent(),
            trailingSpacesTokens[1].value
        )

        val zeroTrailingWhitespaceTokens = assertTokenizesTo(
            """
                %%
                    this on the other hand,
                    should have spaces but no newline at the end    %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            "this on the other hand,\nshould have spaces but no newline at the end    ",
            zeroTrailingWhitespaceTokens[1].value
        )
    }

    @Test
    fun testEmbedBlockTrailingWhitespace() {
        assertTokenizesTo(
            // note the extra whitespace after the opening `%%`
            """
                %%   
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END),
            "should allow trailing whitespace after the opening '%%'"
        )

        assertTokenizesTo(
            // note the extra whitespace after the opening `%%`
            """   
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END),
            "should allow trailing whitespace after the opening '%%embedTag'"
        )
    }

    @Test
    fun testEmbedBlockDanglingDelim() {
        assertTokenizesWithMessages(
            """
            test: %
            """,
            listOf(EMBED_BLOCK_DANGLING_DELIM)
        )
    }

    @Test
    fun testComplexEmbedTagWithWhitespace() {
        assertTokenizesTo(
            """
            %%   this tag has spaces and funky characters ~!@#$%^&*()_+
            some sweet content
            %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )
    }

    @Test
    fun testUnclosedEmbedBlock() {
        assertTokenizesTo(
            """
            %%
            This embed block lacks its closing delimiter
            """,
            listOf(EMBED_START, EMBED_CONTENT)
        )
    }

    @Test
    fun testUnterminatedString() {
        val unclosedStringTokens = assertTokenizesWithMessages(
            """
            "this string has no end quote
            """,
            listOf(STRING_NO_CLOSE)
        )
        assertEquals(listOf(STRING_QUOTE, STRING), unclosedStringTokens.map { it.tokenType })
    }

    @Test
    fun testUnterminatedAltString() {
        val unclosedStringTokens = assertTokenizesWithMessages(
            """
            'this string has no end quote
            """,
            listOf(STRING_NO_CLOSE)
        )
        assertEquals(listOf(STRING_QUOTE, STRING), unclosedStringTokens.map { it.tokenType })
    }

    @Test
    fun testIdentifierLexemeContent() {
        val tokens = assertTokenizesTo(
            """   
                a_key: "a_value"
            """,
            listOf(IDENTIFIER, COLON, STRING_QUOTE, STRING, STRING_QUOTE)
        )

        assertEquals("a_key", tokens[0].value)
        assertEquals("\"", tokens[2].value)
        assertEquals("a_value", tokens[3].value)
    }

    @Test
    fun testTokenLocations() {
        assertTokenizesTo(
            """
            |{
            |    key: val
            |    list: [true, false]
            |    embed: %%
            |      multiline tokens
            |      should have correct
            |      Locations too
            |      %%
            |}
            """.trimMargin(),
            listOf(
                Pair(BRACE_L, Location(0, 0, 0, 1, 0, 1)),
                Pair(IDENTIFIER, Location(1, 4, 1, 7, 6, 9)),
                Pair(COLON, Location(1, 7, 1, 8, 9, 10)),
                Pair(IDENTIFIER, Location(1, 9, 1, 12, 11, 14)),
                Pair(IDENTIFIER, Location(2, 4, 2, 8, 19, 23)),
                Pair(COLON, Location(2, 8, 2, 9, 23, 24)),
                Pair(BRACKET_L, Location(2, 10, 2, 11, 25, 26)),
                Pair(TRUE, Location(2, 11, 2, 15, 26, 30)),
                Pair(COMMA, Location(2, 15, 2, 16, 30, 31)),
                Pair(FALSE, Location(2, 17, 2, 22, 32, 37)),
                Pair(BRACKET_R, Location(2, 22, 2, 23, 37, 38)),
                Pair(IDENTIFIER, Location(3, 4, 3, 9, 43, 48)),
                Pair(COLON, Location(3, 9, 3, 10, 48, 49)),
                Pair(EMBED_START, Location(3, 11, 3, 13, 50, 52)),
                Pair(EMBED_CONTENT, Location(4, 0, 7, 6, 53, 128)),
                Pair(EMBED_END, Location(7, 6, 7, 8, 128, 130)),
                Pair(BRACE_R, Location(8, 0, 8, 1, 131, 132))
            )
        )
    }

    @Test
    fun testStringEscapes() {
        val tokens = assertTokenizesTo(
            """   
                "string with 'unescaped' and \"embedded\" quotes"
            """,
            listOf(STRING_QUOTE, STRING, STRING_QUOTE)
        )

        assertEquals("string with 'unescaped' and \"embedded\" quotes", tokens[1].value)
    }

    @Test
    fun testAltStringEscapes() {
        val tokens = assertTokenizesTo(
            """
                'string with "unescaped" and \'embedded\' quotes'
            """,
            listOf(STRING_QUOTE, STRING, STRING_QUOTE)
        )

        assertEquals("string with \"unescaped\" and \'embedded\' quotes", tokens[1].value)
    }

    @Test
    fun testEmbeddedBlockDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                %%
                these double %\% percents are embedded but escaped%%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("these double %% percents are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testEmbeddedBlockAltDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                $$
                these double $\$ dollars are embedded but escaped$$
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("these double $$ dollars are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testGapFreeLexing() {
        assertTokenizesTo(
            """
                # comment!
                key: val
            """,
            listOf(WHITESPACE, COMMENT, WHITESPACE, IDENTIFIER, COLON, WHITESPACE, IDENTIFIER, WHITESPACE),
            "Should include WHITESPACE tokens when lexing gap-free",
            true
        )

        assertTokenizesTo(
            """
                |  quoted: "string"
                |
            """.trimMargin(),
            listOf(
                Pair(WHITESPACE, Location(0, 0, 0, 2, 0, 2)),
                Pair(IDENTIFIER, Location(0, 2, 0, 8, 2, 8)),
                Pair(COLON, Location(0, 8, 0, 9, 8, 9)),
                Pair(WHITESPACE, Location(0, 9, 0, 10, 9, 10)),
                Pair(STRING_QUOTE, Location(0, 10, 0, 11, 10, 11)),
                Pair(STRING, Location(0, 11, 0, 17, 11, 17)),
                Pair(STRING_QUOTE, Location(0, 17, 0, 18, 17, 18)),
                Pair(WHITESPACE, Location(0, 18, 1, 0, 18, 19))
            ),
            true
        )
    }

    @Test
    fun testCommentPreservation() {
        val tokenList = assertTokenizesTo(
            """
               # a comment
               # another comment
               - 1
               
               # yet another comment
               - 2
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )

        val firstListDashToken = tokenList[0]
        assertEquals(
            2,
            firstListDashToken.comments.size,
            "should have both the comments on this list entry saved with this token"
        )
        assertEquals("# a comment", firstListDashToken.comments[0])
        assertEquals("# another comment", firstListDashToken.comments[1])

        val secondListDashToken = tokenList[2]
        assertEquals(1, secondListDashToken.comments.size)
        assertEquals("# yet another comment", secondListDashToken.comments[0])
    }

    @Test
    fun testTrailingCommentPreservationOnConstants() {
        val tokenList = assertTokenizesTo(
            """
                "stuff" # comment about stuff
            """,
            listOf(STRING_QUOTE, STRING, STRING_QUOTE)
        )

        val endQuoteToken = tokenList[2]
        assertEquals("# comment about stuff", endQuoteToken.comments[0])
    }

    @Test
    fun testTrailingCommentOnLists() {
        val tokenList = assertTokenizesTo(
            """
                [1, # trailing list comma
                2] # trailing list brace
            """,
            listOf(BRACKET_L, NUMBER, COMMA, NUMBER, BRACKET_R)
        )

        val commaToken = tokenList[2]
        assertEquals("# trailing list comma", commaToken.comments[0])

        val rightBracketToken = tokenList[4]
        assertEquals("# trailing list brace", rightBracketToken.comments[0])
    }
}